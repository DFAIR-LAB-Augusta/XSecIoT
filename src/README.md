# ğŸ“‚ `src/`

This directory contains the **source code** for XSecIoT. It is organized into three branches:

* **`core/`** â€” FIRCE: the real-time streaming IDS + Conformal Evaluation engine
* **`FIRE/`** â€” offline preprocessing, modeling, and simulation framework
* **`utils/`** â€” Python utilities for processing/logging experiment outputs

---

## ğŸ” `core/` â€” FIRCE (Streaming Runtime)

The **production-style** pipeline that ingests CICFlowMeter-style flows, classifies in real time, detects concept drift via Conformal Evaluators (ICE/CCE/Approx-CCE/TCE), and logs for adaptive retraining.

**Key modules & scripts**

* **`streaming_pipeline.py`** â€” end-to-end runtime: ingest â†’ preprocess â†’ (PCA/scale) â†’ classify â†’ CE drift detection â†’ log â†’ optional retrain
* **`ce_simulation.py`** â€” batch/stream simulation harness for CE (replay logs, ablations, metrics)
* **`ce_model_training.py`** â€” trains CE-side classifiers used during simulation
* **`adaptive_chunking.py`** â€” adjusts chunk/window sizes in response to runtime conditions
* **`circular_logger.py` / `rolling_csv.py`** â€” high-throughput, size-bounded flow logging
* **`perf_stats.py`** â€” runtime metrics & performance aggregation
* **`config.py`** â€” central configuration for paths/flags used by the core pipeline
* **`run_sim.sh`** â€” one-liner launcher for FIRCE simulation (UV compatible)
* **`run_xseciot.sh`** â€” convenience script to run the end-to-end streaming stack (e.g., with cicflowmeter)

**Models & devices**

* **`models/`**

  * `mlp_ce.py`, `feedforward_binary.py` â€” CE classifier definitions
  * `torch_device.py` â€” CPU/GPU device selection helpers

**Conformal Evaluation**

* **`conformalEval/`**

  * `ice.py`, `cce.py`, `tce.py`, `approx_cce.py` â€” CE variants
  * `conformal_evaluators.py` â€” unified interfaces/wrappers
  * `adaptive_sig_ctlr.py` â€” adaptive significance controller (threshold adaptation)
  * `utils.py` â€” CE helpers (calibration windows, p-values, etc.)
  * `conformal_config.toml` â€” CE configuration (e.g., evaluator, windows, thresholds)

> **I/O expectations:**
>
> * **Input:** CSV flows in `datasets/`
> * **Outputs:** models in `binary_models/` or `multiclass_models/`, logs & metrics in `logging/`

**Quick start (from repo root)**

```bash
uv sync
./src/core/run_sim.sh
```

---

## ğŸ”¬ `FIRE/` â€” Offline Research Framework

The **research-grade** pipeline for dataset preparation, model training, and controlled simulations.

**Key entrypoints**

* **`main.py`** â€” unified CLI for preprocess â†’ train â†’ simulate
* **`preprocessing.py`** â€” cleaning, sessionization, sliding-window aggregation
* **`models.py`** â€” binary & multiclass training; hooks for SHAP/LIME
* **`simulations.py`** â€” sequential/continuous/parallel simulation modes
* **`JuypterNotebooks/`** â€” exploratory analysis (latency, features, model comparisons)

**Example**

```bash
uv run --project . src/FIRE/main.py ./datasets/DFAIR/combined_data_with_okpVacc_modified.csv
```

---

## ğŸ§° `utils/` â€” Log & Metrics Utilities

Helpers for working with run artifacts and logs produced by FIRCE/FIRE.

* **`labeling.py`** â€” programmatic labeling support for flows/segments
* **`merge.py`** â€” safe merges of log shards and intermediate CSVs
* **`overall_perf_stats.py`** â€” compute aggregate metrics across runs (accuracy/F1/runtime, etc.)
* **`overall_stats_scraper.py`** â€” scrape/normalize â€œFull performance stats:â€ blocks from logs

---

## ğŸ“¢ Contact

Seth Barrett | [GitHub](https://github.com/sethbarrett50) | [sebarrett@augusta.edu](mailto:sebarrett@augusta.edu)
Bradley Boswell | [GitHub](https://github.com/bradleyboswell) | [brboswell@augusta.edu](mailto:brboswell@augusta.edu)
Swarnamugi Rajaganapathy, PhD | [GitHub](https://github.com/swarna6384) | [swarnamugi@dfairlab.com](mailto:swarnamugi@dfairlab.com)
Lin Li, PhD | [GitHub](https://github.com/linli786) | [lli1@augusta.edu](mailto:lli1@augusta.edu)
Gokila Dorai, PhD | [GitHub](https://github.com/gdorai) | [gdorai@augusta.edu](mailto:gdorai@augusta.edu)
